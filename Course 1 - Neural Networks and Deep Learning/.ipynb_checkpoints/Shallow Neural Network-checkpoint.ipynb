{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coursera의 ['Deep Learning Specialization'](https://www.coursera.org/specializations/deep-learning#about) 중 Neural Networks and Deep Learning - Shallow Neural Network를 요약한 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing a Neural Network's Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![neural network](https://www.dropbox.com/s/2d64d6m85ylkudx/neural%20network.jpg?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hidden Layer가 하나인 Neural Network에서 Training Sample 하나의 결과 값 $\\hat{y}$를 구하는 식은 다음과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$z^{[1]}_1 = w^{[1]T}_1x + b^{[1]}_1, a^{[1]}_1 = \\sigma(z^{[1]}_1)$$\n",
    "$$z^{[1]}_2 = w^{[1]T}_2x + b^{[1]}_2, a^{[1]}_2 = \\sigma(z^{[1]}_2)$$\n",
    "$$z^{[1]}_3 = w^{[1]T}_2x + b^{[1]}_3, a^{[1]}_3 = \\sigma(z^{[1]}_3)$$\n",
    "$$z^{[1]}_4 = w^{[1]T}_3x + b^{[1]}_4, a^{[1]}_4 = \\sigma(z^{[1]}_4)$$\n",
    "여기서 $z^{[i]}, b^{[i]}, a^{[i]}$의 i는 i번째 Layer를 의미하고, $w^{[i]}_j$의 j는 i번째 Layer의 j번째 노드를 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 식을 Vectorization을 하지 않는 경우에는 for loop를 통해 모든 변수에 대해 계산해야하므로 계산량이 매우 많아진다. 따라서 Vectorization을 통해 계산을 하면 다음과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{bmatrix} w^{[1]}_{11} & w^{[1]}_{12} & w^{[1]}_{13} \\\\ w^{[1]}_{21} & w^{[1]}_{22} & w^{[1]}_{23} \\\\ w^{[1]}_{31} & w^{[1]}_{32} & w^{[1]}_{33} \\\\ w^{[1]}_{41} & w^{[1]}_{42} & w^{[1]}_{43} \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} + \\begin{bmatrix} b^{[1]}_1 \\\\  b^{[1]}_2 \\\\  b^{[1]}_3 \\\\  b^{[1]}_4 \\end{bmatrix} = \\begin{bmatrix} - w^{[1]T}_1 - \\\\ - w^{[1]T}_2 - \\\\ - w^{[1]T}_3 - \\\\ - w^{[1]T}_4 - \\end{bmatrix} \n",
    "\\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} + \\begin{bmatrix} b^{[1]}_1 \\\\  b^{[1]}_2 \\\\  b^{[1]}_3 \\\\  b^{[1]}_4 \\end{bmatrix} = \\begin{bmatrix} w^{[1]T}_1x + b^{[1]}_2 \\\\ w^{[1]T}_1x + b^{[1]}_2 \\\\ w^{[1]T}_3x + b^{[1]}_3  \\\\ w^{[1]T}_4x + b^{[1]}_4 \\end{bmatrix} = \\begin{bmatrix} z^{[1]}_1 \\\\ z^{[1]}_2 \\\\ z^{[1]}_3 \\\\ z^{[1]}_4 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ W^{[1]}_{(4,3)}X_{(3,1)} + b^{[1]}_{(4,1)} = z^{[1]}_{(4,1)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$a^{[1]} = \\begin{bmatrix} a^{[1]}_1 \\\\ a^{[1]}_2 \\\\ a^{[1]}_3 \\\\ a^{[1]}_4 \\end{bmatrix} = \\sigma(z^{[1]}) = \\begin{bmatrix} \\sigma(z^{[1]}_1) \\\\ \\sigma(z^{[1]}_2) \\\\ \\sigma(z^{[1]}_3) \\\\ \\sigma(z^{[1]}_4) \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "간략히 정리하자면 x가 주어졌을 때 결과 값 $\\hat{y}$는"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$z^{[1]} = W^{[1]}x + b^{[1]} = W^{[1]}a^{[0]} + b^{[1]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$a^{[1]} = \\sigma(z^{[1]})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$z^{[2]} = W^{[2]}a^{[1]} + b^{[2]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{y} = a^{[2]} = \\sigma(z^{[2]})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing across multiple examples\n",
    "위에서 하나의 Training Sample에 대해서 결과 값을 계산했다. 여러 개의 Training Sample에 대해서 결과 값을 계산하는 것은 다음과 같다.(2번째 Layer에 대해 계산하는 경우)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$x^{(1)} \\rightarrow a^{[2](1)} = \\hat{y}^{(1)}$$\n",
    "$$x^{(2)} \\rightarrow a^{[2](2)} = \\hat{y}^{(2)}$$\n",
    "$$\\vdots$$\n",
    "$$x^{(i)} \\rightarrow a^{[2](i)} = \\hat{y}^{(i)}$$\n",
    "$$\\vdots$$\n",
    "$$x^{(n)} \\rightarrow a^{[2](n)} = \\hat{y}^{(n)}$$\n",
    "여기서 i는 i번째 Train Sample을 나타내고 n은 Training Sample의 수를 나타낸다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$X_{(3, m)} = \\begin{bmatrix} | & | & \\cdots & | \\\\ x^{(1)} & x^{(2)} & \\cdots & x^{(m)} \\\\ | & | & \\cdots & | \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Z^{[1]}_{(4,m)} = \\begin{bmatrix} | & | & \\cdots & | \\\\ z^{[1](1)} & z^{[1](2)} & \\cdots & z^{[1](m)} \\\\ | & | & \\cdots & | \\end{bmatrix} = \\begin{bmatrix} | & | & \\cdots & | \\\\ W^{[1]}x^{(1)} + b^{[1]} & W^{[1]}x^{(2)} + b^{[1]} & \\cdots & W^{[1]}x^{(m)} + b^{[1]} \\\\ | & | & \\cdots & | \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$A^{[1]}_{(4,m)} = \\begin{bmatrix} | & | & \\cdots & | \\\\ a^{[1](1)} & a^{[1](2)} & \\cdots & a^{[1](m)} \\\\ | & | & \\cdots & | \\end{bmatrix} = \\begin{bmatrix} | & | & \\cdots & | \\\\ \\sigma(z^{[1](1)}) & \\sigma(z^{[1](2)}) & \\cdots & \\sigma(z^{[1](m)}) \\\\ | & | & \\cdots & | \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "간략히 정리하자면 $X$가 주어졌을 때 결과 값 $\\hat{y}$는"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Z^{[1]} = W^{[1]}X + b^{[1]} = W^{[1]}A^{[0]} + b^{[1]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$A^{[1]} = \\sigma(Z^{[1]})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$A^{[2]} = \\sigma(Z^{[2]})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sigmoid](https://www.dropbox.com/s/9201gth3cmk4v3w/sigmoid.jpg?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$a = g(z) = \\frac{1}{1 + e^{-z}}, 0 \\le a \\le 1$$\n",
    "* 이진 분류 시에만 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivative of Sigmoid\n",
    "$$g^\\prime(z) = \\frac{d}{dz}g(z) = \\frac{1}{1+e^{-z}}(1-\\frac{1}{1+e^{-z}})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$z = 10 \\rightarrow g(z) \\approx 1 \\rightarrow g^\\prime(z) \\approx 0$$\n",
    "$$z = -10 \\rightarrow g(z) \\approx 0 \\rightarrow g^\\prime(z) \\approx 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tanh Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tanh](https://www.dropbox.com/s/owkuqrdqdycat0f/tanh.jpg?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$a = g(z) = tanh(z) = \\frac{e^z -e^{-z}}{e^z + e^{-z}}, -1 \\le a \\le 1$$\n",
    "* 대부분의 경우 Sigmoid보다 우월"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivative of tanh\n",
    "$$g^\\prime(z) = \\frac{d}{dz}g(z) = 1 - (tanh(z))^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$z = 10 \\rightarrow tanh(z) \\approx 1 \\rightarrow g^\\prime(z) \\approx 0$$\n",
    "$$z = -10 \\rightarrow tanh(z) \\approx -1 \\rightarrow g^\\prime(z) \\approx 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU Function(Rectified Linear Unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ReLU](https://www.dropbox.com/s/t60bor6wfx6ia9h/relu.jpg?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$a = max(0, z)$$\n",
    "* 가장 흔하게 사용  \n",
    "* 무엇을 쓸지 모를 때 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivative of ReLU\n",
    "$$g^\\prime(z) = \\frac{d}{dz}g(z) = \\begin{cases} 0, & \\text{if z < 0}  \\\\ 1, & \\text{if z > 0} \\\\ \\text{undefined}, & \\text{if z = 0}\\end{cases}$$\n",
    "$$\\text{z = 0인 경우, }\\frac{d}{dz}g(z)를\\text{ 1이나 0으로 지정}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent for Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$n_x = n^{[0]}\\text{(변수의 수)}, n^{[1]}\\text{(첫번째 Hidden Layer의 node 수)}, n^{[2]} = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters : $W^{[1]}_{(n^{[1]}, n^{[0]})}, b^{[1]}_{(n^{[1]}, 1)}, W^{[2]}_{(n^{[2]}, n^{[1]})}, b^{[2]}_{(n^{[2]}, 1)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost Function : J($W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}$) = $\\frac{1}{m}\\sum_{i=1}^n L(\\hat{y}, y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent :  \n",
    "-> Repeat {  \n",
    "        -> Compute Predict : ($\\hat{y^{[i]}}, i = 1, \\cdots, m)$  \n",
    "        Compute Derivative : $dW^{[1]} = \\frac{dJ}{dW^{[1]}}, db^{[1]} = \\frac{dJ}{db^{[1]}}, \\cdots$  \n",
    "        Update Parameters :  \n",
    "        $W^{[1]} := w^{[1]} - \\alpha \\cdot dW^{[1]}$  \n",
    "        $b^{[1]} := b^{[1]} - \\alpha \\cdot db^{[1]}$  \n",
    "        $\\vdots$  \n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "$$Z^{[1]} = W^{[1]}X + b^{[1]} = W^{[1]}A^{[0]} + b^{[1]}$$\n",
    "$$A^{[1]} = \\sigma(Z^{[1]})$$\n",
    "$$Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]}$$\n",
    "$$A^{[2]} = \\sigma(Z^{[2]})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression의 경우\n",
    "![Logistic Regression Gradient Descent](https://www.dropbox.com/s/v1dbvpozhczoyzn/logistic_gradient_descent.jpg?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$da = \\frac{d}{da}L(a, y) = \\frac{d}{da}(-y\\log{a} -(1-y)\\log{(1-a)}) = \\frac{-y}{a} + \\frac{1-y}{1-a}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$dz = \\frac{dL}{dz} = \\frac{dL}{da} \\cdot \\frac{da}{dz} = da \\cdot \\sigma^\\prime(z) = a - y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$dw = dz \\cdot x$$\n",
    "$$db = dz$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\downarrow$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 Hidden Layer Neural Network의 경우\n",
    "![Neural Network Gradient Descent](https://www.dropbox.com/s/nxlfxjzjarw9wbg/neural_network_gradient_descent.jpg?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$dz^{[2]} = a^{[2]} - y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$dW^{[2]} = dz^{[2]}a^{[1]T}$$\n",
    "$$db^{[2]} = dz^{[2]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$dz^{[1]} = W^{[2]T}dz^{[2]} * g^{[1]\\prime}(z^{[1]})$$\n",
    "$$* : \\text{Element-wise product}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$dW^{[1]} = dz^{[1]}x^T$$\n",
    "$$db^{[1]} = dz^{[1]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\downarrow$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$dZ^{[2]} = A^{[2]} - Y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$dw^{[2]} = \\frac{1}{m}dZ^{[2]}A^{[1]T}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$db^{[2]} = \\frac{1}{m}\\text{np.sum(}dZ^{[2]},\\text{ axis = 1, keepdims = True)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$dZ^{[1]} = W^{[2]T}dZ^{[2]} \\times g^{[1]\\prime}(Z^{[1]})$$\n",
    "$$\\times : \\text{element-wise product}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$dW^{[1]} = \\frac{1}{m}dZ^{[1]}X^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$db^{[1]} = \\frac{1}{m}\\text{np.sum(}dZ^{[1]}\\text{, axiss = 1, keepdims = True)}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
