{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coursera의 ['Deep Learning Specialization'](https://www.coursera.org/specializations/deep-learning#about) 중 Neural Networks and Deep Learning - Deep Neural Network를 요약한 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Deep Neural Network](https://www.dropbox.com/s/ief695j340k96xn/deepnn.jpg?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 Network는 Hidden Layer 3개(노드 갯수가 각각 5, 5, 3)와 1개의 Output Layer로 구성된 4-layer Neural Network이다. 이 Network의 Notation은 다음과 같다.\n",
    "* L = 4, Network의 Layer 수\n",
    "* $n^{[l]}$, Layer $l$의 노드 수\n",
    "    * $n^{[1]}$ = 5\n",
    "    * $n^{[2]}$ = 5\n",
    "    * $n^{[3]}$ = 3\n",
    "    * $n^{[L]}$ = 1\n",
    "    * $n^{[0]}$ = $n_x$ = 3, Input Layer의 노드 수\n",
    "* $a^{[l]}$ = $g^{[l]}(z^{[l]})$, Layer $l$의 activation\n",
    "    * $a^{[0]}$ = x(= input features)\n",
    "    * $a^{[L]}$ = $\\hat{y}$\n",
    "* $w^{[l]}$ & $b^{[l]}$, $z^{[l]}$의 weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation in a Deep Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1개의 Training Sample x의 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1번째 Layer  \n",
    "$$z^{[1]} = W^{[1]}x + b^{[1]} = W^{[1]}a^{[0]} + b^{[1]}$$\n",
    "$$a^{[1]} = g^{[1]}(z^{[1]})$$\n",
    "* 2번째 Layer\n",
    "$$z^{[2]} = W^{[2]}a^{[1]} + b^{[2]}$$\n",
    "$$a^{[2]} = g^{[2]}(z^{[2]})$$\n",
    "$\\vdots$\n",
    "* 4번째 Layer\n",
    "$$z^{[4]} = W^{[4]}a^{[3]} + b^{[4]}$$\n",
    "$$a^{[4]} = g^{[4]}(z^{[4]}) = \\hat{y}$$\n",
    "  \n",
    "$$\\downarrow$$\n",
    "  \n",
    "$$z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}$$\n",
    "$$a^{[l]} = g^{[l]}(z^{[l]})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전체 Training Set의 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1번째 Layer\n",
    "$$Z^{[1]} = W^{[1]}X + b^{[1]} = W^{[1]}A^{[0]} + b^{[1]} $$\n",
    "$$A^{[1]} = g^{[1]}(Z^{[1]})$$\n",
    "* 2번째 Layer\n",
    "$$Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]} $$\n",
    "$$A^{[2]} = g^{[2]}(Z^{[2]})$$\n",
    "$\\vdots$\n",
    "* 4번째 Layer\n",
    "$$Z^{[4]} = W^{[4]}A^{[3]} + b^{[4]} $$\n",
    "$$A^{[4]} = g^{[4]}(Z^{[4]}) = \\hat{y}$$\n",
    "  \n",
    "$$\\downarrow$$\n",
    "  \n",
    "$$Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]} $$\n",
    "$$A^{[l]} = g^{[l]}(Z^{[l]})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Z^{[l]} = \\begin{bmatrix} | & | & \\cdots & | \\\\ z^{[1](1)} & z^{[1](2)} & \\cdots & z^{[1](m)} \\\\ | & | & \\cdots & | \\end{bmatrix}$$\n",
    "$$A^{[l]} = \\begin{bmatrix} | & | & \\cdots & | \\\\ a^{[1](1)} & a^{[1](2)} & \\cdots & a^{[1](m)} \\\\ | & | & \\cdots & | \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Your Matrix Dimensions Right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Deep Neural Network2](https://www.dropbox.com/s/5hvar2g2gkozbdd/Deepnn2.jpg?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 Network의 Forward Propagation을 도입해보자.(bias 무시) 우선 Network의 Layer 수와 노드 수는 다음과 같다.  \n",
    "* L = 5  \n",
    "* $n^{[1]}$ = 3\n",
    "* $n^{[2]}$ = 5\n",
    "* $n^{[3]}$ = 4\n",
    "* $n^{[4]}$ = 2\n",
    "* $n^{[5]}$ = 1\n",
    "* $n^{[0]}$ = $n_x$ = 2  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters $W^{[l]}$ and $b^{[l]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z^{[1]}$과 x의 dimension을 살펴보면 다음과 같다.\n",
    "* Dimension$(z^{[1]})$ = (3, 1) = ($n^{[1]}$, 1)\n",
    "* Dimension$(x)$ = (2, 1) = ($n^{[0]}$, 1)\n",
    "\n",
    "따라서 $W^{[1]}$의 dimension은 다음과 같다.\n",
    "* Dimension$(w^{[1]})$ = (3, 2) = ($n^{[1]}$, $n^{[0]}$)\n",
    "\n",
    "위와 같이 $W^{[2]}, \\cdots, W^{[5]}$의 dimension은 다음과 같다.\n",
    "* Dimension$(w^{[2]})$ = (5, 3) = ($n^{[2]}$, $n^{[1]}$)\n",
    "* $\\vdots$\n",
    "* Dimension$(w^{[5]})$ = (1, 2) = ($n^{[5]}$, $n^{[4]}$)\n",
    "\n",
    "이 것을 일반화 하면  \n",
    "$$\\text{Dimension}(W^{[l]}) = (n^{[l]}, n^{[l-1]})$$이 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마찬가지로 $b^{[l]}$의 dimension을 살펴보면\n",
    "* $b^{[1]}$ = (3, 1) = ($n^{[1]}, 1$)\n",
    "* $b^{[2]}$ = (5, 1) = ($n^{[2]}, 1$)\n",
    "* $\\vdots$\n",
    "* $b^{[5]}$ = (1, 1) = ($n^{[5]}, 1$)\n",
    "\n",
    "이 것을 일반화 하면  \n",
    "$$\\text{Dimension}(b^{[l]}) = (n^{[l]}, 1)$$이 되고, dW는 W, db는 b와 같은 dimension을 갖는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorized Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$z^{[1]}_{(n^{[1]}, 1)} = W^{[1]}_{(n^{[1]}, n^{[0]})}x_{(n^{[0]}, 1)} + b^{[1]}_{(n^{[1]}, 1)}$$\n",
    "$$\\downarrow$$\n",
    "$$Z^{[1]}_{(n^{[1]}, m)} = W^{[1]}_{(n^{[1]}, n^{[0]})}X_{(n^{[0]}, m)} + b^{[1]}_{(n^{[1]}, 1)}$$\n",
    "Python의 broadcasting에 의해 $b^{[1]}_{(n^{[1]}, 1)}$은 $b^{[1]}_{(n^{[1]}, m)}$으로 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$z^{[l]}, a^{[l]} : (n^{[l]}, 1)$$\n",
    "  \n",
    "$$Z^{[l]}, A^{[l]} : (n^{[l]}, m)$$\n",
    "$$A^{[0]} = X = (n^{[0]}, m)$$\n",
    "  \n",
    "$$dZ^{[l]}, dA^{[l]} : (n^{[l]}, m)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward and Backward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation for layer $l$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Input : $a^{[l-1]}$\n",
    "* Output : $a^{[l]}, cache (z^{[l]})$\n",
    "$$z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}$$\n",
    "  \n",
    "$$a^{l} = g^{[l]}(z^{[l]})$$\n",
    "  \n",
    "$$\\downarrow$$\n",
    "  \n",
    "$$Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]}$$\n",
    "  \n",
    "$$A^{[l]} = g^{[l]}(Z^{[l]})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Progagation for layer $l$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Input : $da^{[l]}$\n",
    "* Output : $da^{[l-1]}, dW^{[l]}, db^{[l]}$\n",
    "$$dz^{[l]} = da^{[l]} * g^{[l]\\prime}(z^{[l]})$$\n",
    "  \n",
    "$$dW^{[l]} = dz^{[l]}a^{[l-1]}$$\n",
    "  \n",
    "$$db^{[l]} = dz^{[l]}$$\n",
    "  \n",
    "$$da^{[l-1]} = W^{[l]T}dz^{[l]}$$\n",
    "\n",
    "$$\\downarrow$$\n",
    "\n",
    "$$dZ^{[l]} = dA^{[l]} * g^{[l]\\prime}(Z^{[l]})$$\n",
    "\n",
    "$$dW^{[l]} = \\frac{1}{m}dZ^{[l]}A^{[l-1]T}$$\n",
    "\n",
    "$$db^{[l]} = \\frac{1}{m}np.sum(dZ^{[l]}, axis = 1, keepdims = True)$$\n",
    "\n",
    "$$dA^{[l-1]} = W^{[l]T}dZ^{[l]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters vs Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter들이 Parameter들을 컨트롤한다.\n",
    "* Parameters : $W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}, W^{[3]}, b^{[3]}, \\cdots$\n",
    "* Hyperparameters : \n",
    "    * Learning rate $\\alpha$\n",
    "    * Iteration\n",
    "    * Hidden Layer L\n",
    "    * Hidden Units, $n^{[1]}, n^{[2]}, \\cdots$\n",
    "    * Activation function\n",
    "    * Momentum\n",
    "    * Minibatch size\n",
    "    * regularization term\n",
    "    * $\\vdots$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
