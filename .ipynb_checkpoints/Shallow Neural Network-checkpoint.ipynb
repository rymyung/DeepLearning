{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coursera의 ['Deep Learning Specialization'](https://www.coursera.org/specializations/deep-learning#about) 중 Neural Networks and Deep Learning - Shallow Neural Network를 요약한 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing a Neural Network's Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![neural network](https://www.dropbox.com/s/2d64d6m85ylkudx/neural%20network.jpg?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hidden Layer가 하나인 Neural Network에서 Training Sample 하나의 결과 값 $\\hat{y}$를 구하는 식은 다음과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$z^{[1]}_1 = w^{[1]T}_1x + b^{[1]}_1, a^{[1]}_1 = \\sigma(z^{[1]}_1)$$\n",
    "$$z^{[1]}_2 = w^{[1]T}_2x + b^{[1]}_2, a^{[1]}_2 = \\sigma(z^{[1]}_2)$$\n",
    "$$z^{[1]}_3 = w^{[1]T}_2x + b^{[1]}_3, a^{[1]}_3 = \\sigma(z^{[1]}_3)$$\n",
    "$$z^{[1]}_4 = w^{[1]T}_3x + b^{[1]}_4, a^{[1]}_4 = \\sigma(z^{[1]}_4)$$\n",
    "여기서 $z^{[i]}, b^{[i]}, a^{[i]}$의 i는 i번째 Layer를 의미하고, $w^{[i]}_j$의 j는 i번째 Layer의 j번째 노드를 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 식을 Vectorization을 하지 않는 경우에는 for loop를 통해 모든 변수에 대해 계산해야하므로 계산량이 매우 많아진다. 따라서 Vectorization을 통해 계산을 하면 다음과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{bmatrix} w^{[1]}_{11} & w^{[1]}_{12} & w^{[1]}_{13} \\\\ w^{[1]}_{21} & w^{[1]}_{22} & w^{[1]}_{23} \\\\ w^{[1]}_{31} & w^{[1]}_{32} & w^{[1]}_{33} \\\\ w^{[1]}_{41} & w^{[1]}_{42} & w^{[1]}_{43} \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} + \\begin{bmatrix} b^{[1]}_1 \\\\  b^{[1]}_2 \\\\  b^{[1]}_3 \\\\  b^{[1]}_4 \\end{bmatrix} = \\begin{bmatrix} - w^{[1]T}_1 - \\\\ - w^{[1]T}_2 - \\\\ - w^{[1]T}_3 - \\\\ - w^{[1]T}_4 - \\end{bmatrix} \n",
    "\\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} + \\begin{bmatrix} b^{[1]}_1 \\\\  b^{[1]}_2 \\\\  b^{[1]}_3 \\\\  b^{[1]}_4 \\end{bmatrix} = \\begin{bmatrix} w^{[1]T}_1x + b^{[1]}_2 \\\\ w^{[1]T}_1x + b^{[1]}_2 \\\\ w^{[1]T}_3x + b^{[1]}_3  \\\\ w^{[1]T}_4x + b^{[1]}_4 \\end{bmatrix} = \\begin{bmatrix} z^{[1]}_1 \\\\ z^{[1]}_2 \\\\ z^{[1]}_3 \\\\ z^{[1]}_4 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ W^{[1]}_{(4,3)}X_{(3,1)} + b^{[1]}_{(4,1)} = z^{[1]}_{(4,1)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$a^{[1]} = \\begin{bmatrix} a^{[1]}_1 \\\\ a^{[1]}_2 \\\\ a^{[1]}_3 \\\\ a^{[1]}_4 \\end{bmatrix} = \\sigma(z^{[1]}) = \\begin{bmatrix} \\sigma(z^{[1]}_1) \\\\ \\sigma(z^{[1]}_2) \\\\ \\sigma(z^{[1]}_3) \\\\ \\sigma(z^{[1]}_4) \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "간략히 정리하자면 x가 주어졌을 때 결과 값 $\\hat{y}$는"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$z^{[1]} = W^{[1]}x + b^{[1]} = W^{[1]}a^{[0]} + b^{[1]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$a^{[1]} = \\sigma(z^{[1]})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$z^{[2]} = W^{[2]}a^{[1]} + b^{[2]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{y} = a^{[2]} = \\sigma(z^{[2]})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing across multiple examples\n",
    "위에서 하나의 Training Sample에 대해서 결과 값을 계산했다. 여러 개의 Training Sample에 대해서 결과 값을 계산하는 것은 다음과 같다.(2번째 Layer에 대해 계산하는 경우)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$x^{(1)} \\rightarrow a^{[2](1)} = \\hat{y}^{(1)}$$\n",
    "$$x^{(2)} \\rightarrow a^{[2](2)} = \\hat{y}^{(2)}$$\n",
    "$$\\vdots$$\n",
    "$$x^{(i)} \\rightarrow a^{[2](i)} = \\hat{y}^{(i)}$$\n",
    "$$\\vdots$$\n",
    "$$x^{(n)} \\rightarrow a^{[2](n)} = \\hat{y}^{(n)}$$\n",
    "여기서 i는 i번째 Train Sample을 나타내고 n은 Training Sample의 수를 나타낸다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$X_{(3, m)} = \\begin{bmatrix} | & | & \\cdots & | \\\\ x^{(1)} & x^{(2)} & \\cdots & x^{(m)} \\\\ | & | & \\cdots & | \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Z^{[1]}_{(4,m)} = \\begin{bmatrix} | & | & \\cdots & | \\\\ z^{[1](1)} & z^{[1](2)} & \\cdots & z^{[1](m)} \\\\ | & | & \\cdots & | \\end{bmatrix} = \\begin{bmatrix} | & | & \\cdots & | \\\\ W^{[1]}x^{(1)} + b^{[1]} & W^{[1]}x^{(2)} + b^{[1]} & \\cdots & W^{[1]}x^{(m)} + b^{[1]} \\\\ | & | & \\cdots & | \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$A^{[1]}_{(4,m)} = \\begin{bmatrix} | & | & \\cdots & | \\\\ a^{[1](1)} & a^{[1](2)} & \\cdots & a^{[1](m)} \\\\ | & | & \\cdots & | \\end{bmatrix} = \\begin{bmatrix} | & | & \\cdots & | \\\\ \\sigma(z^{[1](1)}) & \\sigma(z^{[1](2)}) & \\cdots & \\sigma(z^{[1](m)}) \\\\ | & | & \\cdots & | \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "간략히 정리하자면 $X$가 주어졌을 때 결과 값 $\\hat{y}$는"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Z^{[1]} = W^{[1]}X + b^{[1]} = W^{[1]}A^{[0]} + b^{[1]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$A^{[1]} = \\sigma(Z^{[1]})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$A^{[2]} = \\sigma(Z^{[2]})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$a = \\frac{1}{1 + e^{-z}}, 0 \\le a \\le 1$$\n",
    "이진 분류 시에만 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tanh Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$a = tanh(z) = \\frac{e^z -e^{-z}}{e^z + e^{-z}}, -1 \\le a \\le 1$$\n",
    "양 끝단의 미분 값이 0에 가까움  \n",
    "Sigmoid보다 우월"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU Function(Rectified Linear Unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$a = max(0, z)$$\n",
    "가장 흔하게 사용  \n",
    "무엇을 쓸지 모를 때  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why do you need non-linear activation functions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
