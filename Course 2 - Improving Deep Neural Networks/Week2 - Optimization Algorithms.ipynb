{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coursera의 ['Deep Learning Specialization'](https://www.coursera.org/specializations/deep-learning#about) 중 Improving Deep Neural Network : Hyperparameter tuning, Regularization and Optimization - Optimization Algorithms를 요약한 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "머신러닝을 적용하는 것은 여러 모델들을 학습시키고 가장 잘 작동하는 것을 찾는 반복적인 작업이다. 그렇기 때문에 빠르게 모델들을 학습시키는 것이 매우 중요하다. 그러나 Deep Learning은 데이터가 클 수록 잘 작동하는 경향이 있기 때문에 문제가 발생한다. 이런 경우 빠른 최적화 알고리즘과 양질의 최적화 알고리즘을 갖는 것이 효율성을 높일 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch vs Mini-batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Batch Gradient Descent : 이전에 다뤘던 Gradient Descent Algorithm으로 전체 Training set를 한 번에 처리하는 방법\n",
    "* Mini-batch Gradient Descent : single mini batch인 $X^{\\{t\\}}, Y^{\\{t\\}}$를 한 번에 처리하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Mini Batch](https://www.dropbox.com/s/p64wv8f7b8nhkfa/minibatch.jpg?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$X_{(n_x, m)} = \\begin{bmatrix} x^{(1)} & x^{(2)} & \\cdots & x^{(m)} \\end{bmatrix}$$  \n",
    "$$Y_{(1, m)} = \\begin{bmatrix} y^{(1)} & y^{(2)} & \\cdots & y^{(m)} \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만약 m이 5천만이고 데이터를 1000개씩 묶으면 다음과 같이 된다.\n",
    "$$X_{(n_x, m)} = \\begin{bmatrix} x^{(1)} & x^{(2)} & \\cdots & x^{(m)} \\end{bmatrix} = \\begin{bmatrix} X^{(1)}_{(n_x, 1000)} & X^{(2)}_{(n_x, 1000)} & \\cdots & X^{(5000)}_{(n_x, 1000)} \\end{bmatrix}$$  \n",
    "$$Y_{(1, m)} = \\begin{bmatrix} y^{(1)} & y^{(2)} & \\cdots & y^{(m)} \\end{bmatrix} = \\begin{bmatrix} Y^{(1)}_{(1, 1000)} & Y^{(2)}_{(1, 1000)} & \\cdots & Y^{(5000)}_{(1, 1000)} \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 데이터를 가지고 Mini-batch Gradient Descent Algorithm을 적용하면 다음과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* for t = 1 to 5000 : \n",
    "    1. Implement foraward propagation on $X^{\\{t\\}}$ using Vectorization(1000 examples)\n",
    "        * $Z^{[1]} = W^{[1]}X^{\\{t\\}} + b^{[1]}$\n",
    "        * $A^{[1]} = g^{[1]}(Z^{[1]})$\n",
    "        * $\\vdots$\n",
    "        * $A^{[L]} = g^{[L]}(Z^{[L]})$\n",
    "    2. Compute cost $ J^{\\{t\\}} = \\frac{1}{1000}\\sum_{i=1}^l L(\\hat{y}^{(i)}, y^{(i)}) + \\frac{\\lambda}{2 \\cdot 1000}\\sum_l \\left\\vert\\left\\vert w \\right \\vert\\right \\vert_F^2$\n",
    "    3. Implement Backward propagation to compute gradient cost $J^{\\{t\\}}$ (Using only $X^{\\{t\\}}, Y^{\\{t\\}}$)\n",
    "    4. Update parameters $W^{[l]} = W^{[l]} - \\alpha \\cdot dW^{[l]}, b^{[l]} = b^{[l]} - \\alpha \\cdot db^{[l]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose Mini-batch Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Mini-batch size = m : Batch Gradient Descent $\\rightarrow (X^{\\{t\\}}, Y^{\\{t\\}}) = (X, Y)$\n",
    "* Mini-batch size = 1 : Stochastic Gradient Descent $\\rightarrow (X^{\\{t\\}}, Y^{\\{t\\}}) = (X^{(1)}, Y^{(1)}) \\cdots (X^{(m)}, Y^{(m)})$ every example is mini batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![batchsize](https://www.dropbox.com/s/qpxcazq6mv2puiy/batchsize.jpg?raw=1)\n",
    "Batch Gradient Descent의 경우, 비교적 낮은 noise와 큰 step으로 최소값을 찾아간다. 그러나 매 Iteration마다 긴 시간이 소요된다.\n",
    "Stochastic Gradient Descent는 noise가 크고 최소값 범위에서 왔다갔다하지만 noise는 더 작은 값의 learning rate를 사용하여 개선시킬 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mini-batch Size를 고르는 기준은 다음과 같다.\n",
    "* 데이터 크기가 작다면, Batch Gradient Descent를 사용($m \\le 2000$)\n",
    "* 데이터 크기가 크다면, Mini-batch size는 일반적으로 64, 128, 256, 512가 대표적으로 사용된다. 컴퓨터 메모리가 배치되어 있는 방식과 접속되는 방법에 따라 때때로 2의 지수값을 가질 때 더 빨리 실행된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent with Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mini-batch Gradient Descent는 Train set의 일부만 가지고 parameter를 업데이트하기 때문에, 업데이트의 방향의 약간의 variance를 가지고 있고 어느정도 진동을 하면서 최소점으로 향한다. Momentum은 이전의 Gradient를 고려하여 업데이트하기 때문에 최점으로 향할 때 생기는 진동을 스무스하게 만들어주고, 일반적인 Gradient Descent Algorithm보다 거의 더 빨리 작동한다. 기본 아이디어는 기울기의 Exponentially Weighted Average를 통해 기울기 강하의 절차를 스무스하게 해준다.\n",
    "$$V_{dw} = \\beta_1 \\cdot V_{dw} + (1 - \\beta_1) \\cdot dW$$\n",
    "$$V_{db} = \\beta_1 \\cdot V_{db} + (1 - \\beta_1) \\cdot db$$\n",
    "* $\\beta_1$을 크게 할수록 이전의 Gradient를 많이 고려하게 되어 업데이트가 더 스무스해진다\n",
    "* $\\beta_1$는 0에서 1 사이의 값으로 보통 0.9를 흔하게 사용한다.\n",
    "\n",
    "\n",
    "그 다음에 이 기울기를 이용해서 weights를 업데이트한다.\n",
    "$$W := W - \\alpha \\cdot V_{dw}$$\n",
    "$$b := b - \\alpha \\cdot V_{db}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Root Mean Square Propagation의 약자로 이 방법을 이용하여 Gradient Descent의 속도를 증가시킬 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$S_{dw} = \\beta_2 \\cdot S_{dw} + (1 - \\beta_2) \\cdot dW^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$S_{db} = \\beta_2 \\cdot S_{db} + (1 - \\beta_2) \\cdot db^2$$\n",
    "Adam Algorithm 발명자는 $\\beta_2$를 0.999로 권장했다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$W := W - \\alpha \\frac{dW}{\\sqrt{S_{dw}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$b := b - \\alpha \\frac{db}{\\sqrt{S_{db}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam Optimization Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam Algorithm은 Nerual Network를 학습시킬 때 가장 효율적인 Optimization Algorithm 중 하나로 기본적으로 Momentum과 RmsProp을 같이 합치는 것이다. \n",
    "$$V_{dw} = 0, S_{dw} = 0, V_{db} = 0, S_{db} = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$V_{dw} = \\beta_1 \\cdot V_{dw} + (1 - \\beta_1) \\cdot dW$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$V_{db} = \\beta_1 \\cdot V_{db} + (1 - \\beta_1) \\cdot db$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$S_{dw} = \\beta_2 \\cdot S_{dw} + (1 - \\beta_2) \\cdot dW^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$S_{db} = \\beta_2 \\cdot S_{db} + (1 - \\beta_2) \\cdot db^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$V^{corrected}_{dw} = \\frac{V_{dw}}{(1 - \\beta^t_1)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$V^{corrected}_{db} = \\frac{V_{db}}{(1 - \\beta^t_1)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$S^{corrected}_{dw} = \\frac{S_{dw}}{(1 - \\beta^t_2)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$S^{corrected}_{db} = \\frac{S_{db}}{(1 - \\beta^t_2)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$W := W - \\alpha \\cdot \\frac{V^{corrected}_dw}{\\sqrt{S^{corrected}_dw} + \\epsilon}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$b := b - \\alpha \\cdot \\frac{V^{corrected}_db}{\\sqrt{S^{corrected}_db} + \\epsilon}$$\n",
    "$\\epsilon$은 주로 $10^{-8}$을 많이 사용한다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
