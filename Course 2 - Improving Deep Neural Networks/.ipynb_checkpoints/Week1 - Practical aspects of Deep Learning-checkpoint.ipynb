{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coursera의 ['Deep Learning Specialization'](https://www.coursera.org/specializations/deep-learning#about) 중 Improving Deep Neural Network : Hyperparameter tuning, Regularization and Optimization - Practical aspects of Deep Learning를 요약한 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias and Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias(편향)과 Variance(편차)에 대해서 이해하기 위한 2가지 핵심 수치는\n",
    "* Train set error\n",
    "* Development set error\n",
    "이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만약 Train set error가 1%, Dev set error가 11%이고 일반적으로 이 문제의 error가 1%인 경우, Train set에 Overfitting됐다고 볼 수 있다. 이 경우에 큰 편차(High Variance)를 갖고 있다고 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만약 Train set error가 15%, Dev set error가 16%이고 일반적으로 이 문제의 error가 1%인 경우, 모델이 Train set에도 잘 작동하지 않는 Underfitting됐다고 볼 수 있다. 이 경우에 큰 편향(High Bias)를 갖고 있다고 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High Bias & High Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만약 Train set error가 15%, Dev set error가 30%이고 일반적으로 이 문제의 error가 1%인 경우, 큰 편향(High Bias)과 큰 편차(High Variance)를 모두 갖고 있다고 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low Bias & Low Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만약 Train set error가 0.5%, Dev set error가 1%이고 일반적으로 이 문제의 error가 1%인 경우, 큰 편향(High Bias)과 큰 편차(High Variance)를 모두 갖고 있다고 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Recipe for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "초기 모델을 학습한 이후 시점에 확인할 사항들\n",
    "1. High Bias 여부 : Train set과 Train set의 성능\n",
    "    * 만약 High Bias를 띄고있다면,\n",
    "        * 더 많은 Hidden Layer와 Hidden Unit\n",
    "        * 더 오래 학습\n",
    "        * 문제에 더 적합한 새로운 신경망 구조 찾기(CNN, RNN, ETC...)\n",
    "2. High Variance 여부 : Dev set의 성능\n",
    "    * 만약 High Variance를 띄고있다면,\n",
    "        * 더 많은 데이터 수집\n",
    "        * Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High Variance를 띄고있을 때 시도해야 할 일 중 하나로 정규화(Regularization)이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "로지스틱 회귀를 사용하여 정규화를 살펴보자. 비용함수 J를 최소화하도록 시도한다. \n",
    "$$ J(w,b) = \\frac{1}{m}\\sum_i^{m}L(\\hat{y}^{(i)}, y^{(i)}) $$\n",
    "$$ w \\in \\mathbb{R}^{n_x}, b \\in \\mathbb{R}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "로지스틱 회귀에 정규화를 더하기 위해서는 Lambda라고 불리는 조절 매개 변수를 추가해야한다.\n",
    "$$ J(w,b) = \\frac{1}{m}\\sum_i^{m}L(\\hat{y}^{(i)}, y^{(i)}) + \\frac{\\lambda}{2m}\\left\\vert\\left\\vert w \\right\\vert\\right\\vert_2^2$$\n",
    "$$L_2 \\text{ regularization} : \\left\\vert\\left\\vert w \\right\\vert\\right\\vert_2^2 = \\sum_j^{n_x} w_j^2 = w^Tw$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L_1$ regularization을 적용하면 다음과 같다.\n",
    "$$ J(w,b) = \\frac{1}{m}\\sum_i^{m}L(\\hat{y}^{(i)}, y^{(i)}) + \\frac{\\lambda}{m}\\left\\vert\\left\\vert w \\right\\vert\\right\\vert_1$$\n",
    "$$L_1 \\text{ regularization} : \\left\\vert\\left\\vert w \\right\\vert\\right\\vert_1 = \\sum_j^{n_x}\\left\\vert w_j \\right\\vert$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "매개 변수 w만 정규화를 하고 b에 대해서는 안하는 이유 : w는 보통 꽤 높은 차원의 parameter vector이다. 특히 Variance가 심한 문제일 시에는 더욱 그런 반면에, b는 그저 single number일 뿐이다. 그래서 거의 모든 매개 변수들이 b보다는 w에 있기 때문에 b는 생략하는 경우가 많다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network에 정규화를 적용하면 다음과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(w^{[1]}, b^{[1]}, \\cdots, w^{[L]}, b^{[L]} = \\frac{1}{m}\\sum_i^nL(\\hat{y}^{(i)}, y^{(i)}) + \\frac{\\lambda}{2m}\\sum_l^L\\left\\vert\\left\\vert w^{[l]} \\right\\vert\\right\\vert_F^2$$\n",
    "$$\\text{Frobenius norm, F} : \\left\\vert\\left\\vert w^{[l]} \\right\\vert\\right\\vert_2^2 = \\sum_i^{n^{[l-1]}}\\sum_j^{n^{[l]}}\\left(w_{ij}^{[l]}\\right)^2,  w : (n^{[l]}, n^{[l-1]})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$dW^{[l]} = (\\text{from backprop}) + \\frac{\\lambda}{m}W^{[l]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Regularization Reduces Overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "직관적으로 $\\lambda$ 값을 크게 설정하면, $W^{[l]} \\approx 0$이 되므로 수많은 Hidden Unit에 대하여 Weight를 거의 0에 가까운 값으로 지정하여 Hidden Unit의 영향을 많이 줄여준다. 그렇게 되면 원래의 Network보다 더 단순한 Network이 되어 overfitting한 경우에서 High Bias를 갖는 underfitting한 경우로 이동하게 된다. $\\lambda$ 값을 적절하게 설정해주어서 Underfitting이 되기 전에 이상적인 케이스가 되도록 해주어야한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![regualarzation_ex](https://www.dropbox.com/s/xjf71305cxuqadr/regularization.jpg?raw=1)\n",
    "activation function이 tanh인 경우를 예로 살펴보자. tanh 함수는 z가 0에 가까운 값인 범위에서 대략적으로 선형 형태를 띈다. 만약 $\\lambda$ 값을 크게 설정하면 $W^{[l]}$은 0에 가깝게 되고 따라서 $z^{[l]}$ 또한 0에 가까운 값이 된다. 이 것은 마치 모든 Layer가 대략적으로 선형인 것과 비슷하다. 따라서 복잡한 비선형 형태의 의사결정은 불가능하게 되어 overfitting이 불가능하게 만든다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout Regualarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout은 $L_2$ regularization보다 더욱 강력한 Regularzation 기법이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dropout](https://www.dropbox.com/s/dskttimslmr3j2r/dropout.jpg?raw=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout은 Network의 각 Layer별로 확률적으로 Node를 제거한다. 그렇게하면 원래 Network보다 더 작은 Network가 되어 Overfitting을 막아준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Dropout : Inverted Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3번째 layer를 통해 Dropout을 적용하는 방법을 살펴보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keep_prob은 Node를 제거할 확률을 나타낸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d3 = np.random.rand(a3.shape[0], a3.shape[1]) < keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a^3$와 동일한 크기의 1, 0 matrix를 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a3 = np.multiply(a3, d3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a^3$와 $d^3$를 element-wise 곱을 하여 a3 Node의 20%를 0으로 만들어준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a3 /= keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z^4$의 기대값을 감소시키지 않게 하기 위해 $a^3$에 keep_prob을 나눠준다. 이 과정을 통해서 $a^3$에서 사라진 20%의 Node의 값을 다시 20%만큼 올려 $a^3$의 기대값을 감소시키지 않는다. 이 부분을 **Invertd Dropout Technique**라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Regularization Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Augmentation](https://www.dropbox.com/s/qr63jb2y3gq867t/augmentation.jpg?raw=1)\n",
    "데이터가 Image인 경우, 좌우반전/회전 등을 통해 데이터의 양을 늘릴 수 있다. 이런 경우, 데이터가 어느정도 중복되기 때문에 새로운 데이터를 추가하는 것보다 좋지는 않지만 데이터를 손쉽게 추가할 수 있는 장점이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Early Stopping](https://www.dropbox.com/s/k2uxawo4pq202so/earlystopping.jpg?raw=1)\n",
    "Train error와 Dev error Plot을 그려서 Dev Error가 상승하는 지점에서 Training을 멈추는 방법이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization(표준화)는 Neural Network를 훈련시킬 때, 학습 속도를 높이는 방법 중 하나이다. 특히 Input Features의 scale이 매우 다른 경우 Normalization은 매우 중요하다.\n",
    "$$Z = \\frac{X-mean(X)}{sd(X)}$$\n",
    "Train set과 Test set을 따로따로 Normalization 하는 것은 좋지 않다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Normalization](https://www.dropbox.com/s/qbhvtfadbag9vzc/normalization.jpg?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization을 하게되면 비용함수가 평균적으로 더 대칭 성향을 띄우는 모양이 된다. Normalization을 안했을 때의 비용함수에 Gradient Descent를 적용하면 매우 작은 Learning rate를 사용해야 한다. 반면에 Normalization을 했을 때의 비용함수에 Gradient Descent를 적용하면 비용함수가 좀 더 구형의 윤곽선을 띄고 있어 어디서 시작하든 손쉽게 최소점에 도달할 수 있어 보다 큰 Learning rate를 사용해도 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanishing / Exploding Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Neural Network를 학습시킬 때 가장 큰 문제점들 중에 다음과 같은 것들이 있다.\n",
    "* 데이터가 사라지는 경우\n",
    "* 기울기가 폭발적으로 증가하는 경우\n",
    "이는 학습을 하는 동안 Derivative나 기울기가 굉장히 큰 값을 갖게되거나 굉장히 작은 값을 갖게 되는 경우이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Initialization for Deep Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight의 임의 초기 선정을 좀 더 조심스럽게 하여 Vanishing / Exploding Gradients 문제를 부분적으로 해결할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Activation Function이 ReLU일 때 : $W^{[l]}$ = np.random.rand(shape) * np.sqrt($\\frac{2}{n^{[l-1]}}$)\n",
    "* Activation Function이 tanh일 때 : $W^{[l]}$ = np.random.rand(shape) * np.sqrt($\\frac{1}{n^{[l-1]}}$)  \n",
    "$n^{[l-1]}$ : $l-1$ Layer의 Node  수"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
